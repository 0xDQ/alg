% ----------------------------------------------------------------
% AMS-LaTeX Paper ************************************************
% **** -----------------------------------------------------------
\documentclass{amsart}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{inconsolata}
\usepackage[shortlabels]{enumitem}
% \usepackage{xy}
% ----------------------------------------------------------------
\vfuzz2pt % Don't report over-full v-boxes if over-edge is small
\hfuzz2pt % Don't report over-full h-boxes if over-edge is small
% THEOREMS -------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem*{lemstar}{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{claim}[thm]{Claim}
\newtheorem*{claimstar}{Claim}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{alg}[thm]{Algorithm}
\newtheorem*{algstar}{Algorithm}
\theoremstyle{remark}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{rmkstar}{Remark}
\numberwithin{equation}{section}
% MATH -----------------------------------------------------------
\newcommand{\Matrix}[4]{ \left( \begin{array}{cc}  #1 & #2 \\  #3 & #4 \\ \end{array} \right) }
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\NN}{\mathbb N}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\QQ}{\mathbb Q}
\newcommand{\RR}{\mathbb R}
\newcommand{\CC}{\mathbb C}
\newcommand{\isom}{\cong}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\let\null\varnothing

\pagestyle{plain}
% ----------------------------------------------------------------
\begin{document}
\title[]{Algorithms Homework 1}%
\author{Evan Simmons \\
        Dept. of Mathematics \& Computer Science \\ University of California Santa Cruz}%
%\date{}
%\dedicatory{}%
%\commby{}%
\renewcommand{\abstractname}{Homework Option}
% ----------------------------------------------------------------
\begin{abstract}
I have chosen the homework heavy option.
\end{abstract}
\maketitle
% ----------------------------------------------------------------

\section{} Let $G = G(V, E)$ be an arbitrary, connected, undirected graph with
vertex set $V$ and edge set $E$, where each $e \in E$ has a distinct
positive length le. Let $f : \RR^+ \rightarrow \RR^+$ be strictly
increasing. We want to compute a subset of edges $S \subseteq E$ such
that:

\begin{enumerate}[(a)]
  \item $G(V,S)$ is connected.
  \item $\sum_{e \in S} f(l_e)$ is as small as possible.
\end{enumerate}

What is the smallest number of evaluations of $f$ needed to do this?

\claimstar We need not evaluate $f$ at all. Further, finding the MST
with respect to the $l_e$'s is equivalent to finding the MST with
respect to $f(l_e)$.

\proof Consider the sorted sequences of lengths $L$ and outputs of
functions $F$. It has been shown that Kruskal's algorithm determines
the minimal spanning tree for a weighted graph. By the definition of
strictly increasing,
$$ x_1 < x_2\ \Rightarrow\ f(x_1) < f(x_2), $$
the ordering of the inputs is preserved. Since kruskal's algorithm
takes the same first $k$ edges, it produces identical trees. Therefore
we need only compute the MST with Kruskal's where edge weights equal edge
length.

\section{} Give an algorithm that given a sequence of $n$ positive real numbers $W = w_1, w_2,
\ldots, w_n$ and a real number $B$ partitions $W$ into contiguous subsequences, i.e,
$$ S = \{ (w_1,w_2,\ldots,w_k), (w_{k+1},w_{k+2},\ldots,w_l),\ldots,(w_q,w_{q+1},\ldots,w_n) \} $$

such that the sum of the numbers in each subsequence is at most $B$ while
minimizing the number of subsequences. (Assume $w_i \leq B$.)

\algstar We iterate through the sequence of $w$'s in order. If our set
of subsequences is empty, we create a new subsequence and append the
current element. Otherwise, for each $w_i$, we attempt to add it to
the last subsequence in $S$. If this fails by making the sum of the
subsequence more than $B$, we instead create a new subsequence and append
$w_i$.

\proof If $W = \null$ then we have a set with an empty sequence and
are done. Assume towards a contradiction that for some $W$ we have an
optimal solution $O$ which differs from the solution produced by the
above algorithm $S$. Let $w_j$ denote the element of first difference,
which corresponds to the subsequence that $w_j$ is placed in. There are
two cases, in the first:
\begin{align*}
  S &= \{ \ldots, \overbrace{\{w_k, \ldots  w_{j-1}\} }^{S_i}, \{w_j, w_{j+1},\ldots \}, \ldots \} \\
  O &= \{ \ldots, \underbrace{\{w_k, \ldots w_{j-1}, w_j \}}_{O_i}, \{w_{j+1},\ldots \}, \ldots \}.
\end{align*}

But we immediately see that the sum of $O_i > B$ by the definition of our
algorithm.$\Rightarrow \Leftarrow$

In the second case:
\begin{align*}
  S &= \{ \ldots, \overbrace{\{w_k, \ldots  w_{j-1}, w_j \} }^{S_i}, \{w_{j+1},\ldots \}, \ldots \} \\
  O &= \{ \ldots, \underbrace{\{w_k, \ldots w_{j-1} \}}_{O_i}, \{ w_j, w_{j+1},\ldots \}, \ldots \}.
\end{align*}

But then we have shown that for the first $j$ elements, our algorithm does better than
that of optimal. $\Rightarrow \Leftarrow$

Hence the output of our algorithm is the unique optimal solution.

\rmkstar Clearly our algorithm runs in $O(n)$ time since we traverse the
set $W$ once.

\section{}

We wish to manufacture n distinct hardware items. Each item needs to go
through 3 stages of processing. The first, and most important stage,
called \textit{design} can only be performed by our master designer who works
by starting work on an item, designing it through to the end, and only
then starting on another item. The remaining two phases, called \textit{assembly}
and \textit{testing}, are outsourced and for each of them there is an infinite
supply of people who can perform the corresponding task as soon as it is
assigned to them. Naturally, each item first needs to be designed, then
assembled, and then tested.

Each item $i$ requires $d_i$ hours of design, $a_i$ hours of assembly, and $t_i$
hours of testing. We are interested in determining the order in which we
should design the items so that we minimize the time by which all items
will be ready.

Give a $O(n\ log\ n)$ algorithm which takes as input $n$ triples
$(d_i,a_i,t_i)$ and determines the optimal design order.

\claimstar It is sufficient to start the jobs in order of the sum of
their respective assembly and testing times.

\proof Let us call each product's $assembly + test$ quantity it's
\textit{tail}. Suppose there exists an optimal schedule $\sigma$ such
that the design phases are not ordered in descending size of each
product's \textit{tail}.

Consider the last designed product $p$ and any preceding products
which have a smaller \textit{tail}. Of those with a smaller tail take
the one with the least \textit{tail} $p'$ and swap its scheduling
order with $p$. We see that the new schedule $\sigma'$ is no longer
than $\sigma$. If we apply this swapping recursively on the prefix
of $\sigma'$, we end up with schedule such that each job is ordered
descendingly with respect to the size of its tail and takes no more
time than $\sigma$.

\alg We employ mergesort with the comparator function that returns the
comparison of the sum of the assembly and testing times of each element.
Since the sum and compare operations are independent of input size, this
runs in $O(n\ log\ n)$

\section{} Let $G = G(V, E)$ be a directed graph representing a
road network. That is, an arc $(u, v)$ from vertex $u$ to vertex
$v$ corresponds to a road connecting location $u$ with location $v$
directly. Going beyond the standard shortest-path setting, we will now
also take into account traffic by accepting that the amount of time it
takes to cross an arc $e = (u, v)$ depends on the time that one starts
of at $u$. Specifically, we will assume that based on past observations
of traffic patterns we have for each arc $e = (u, v)$ a function $g_e$
which takes as input the time one leaves $u$ and returns the time that
they will arrive at $v$. The functions are such that for every arc $e$,
we have $g_e(t) \geq t$ for every time $t$ and $g_e(t_1) < g_e(t_2)$,
whenever $t_1 < t_2$.

Modify Dijkstraâ€™s algorithm to get a $O(|E|\cdot log\ |E|)$ algorithm which
finds the soonest that one can reach a given vertex $\textbf{u}$ of $G$ if they
leave a given vertex $s$ of $G$ at a given time $t_0$.

\claimstar We can simply use Dijkstra's algorithm (calling distance
time) since $g_e(t) \geq t$ and strictly increasing are exactly the
properties that enabled Dijkstra's in the first place.

% \proof The problem given is exactly that solved by Dijkstra's except
% since we are dealing with time, exploring a node at different times
% results in different weights. Specifically there could exist a shorter
% path between an arbitrary node and the next node by "delaying" arrival
% time.

\algstar{DIJKSTRA\_TIME} We maintain a set of explored nodes $S$ for
which we have determined the shortest time to arrive at that node from
$s$. Initialize $S=\{s\}$, and $t(s) = t_0$. Let the current node be
denoted by $v$. We consider the nodes that its edges touch, and the
resulting total times from $s$ to the new nodes. If a newly explored
edge yields a shorter time to a vertex already explored, record the
new time. Add newly explored vertices (and their discovered path time)
to the list of explored vertices. If $u$ is the node with the shortest
path, return the path time to $u$. Otherwise set the current node to
that with the shortest path from $s$ and recurse.


% If all edges have been.
% Otherwise we consider the weights of all the edges $E_v$ emanating
% from the given vertex $v$. Let $t$ be the time at which we reached $v$
% (which is starts at $t_0$. We then add $e_i$ to the list of explored nodes
% where: 
% $$ e_i \in E_v \text{ and } g_{e_i}(t) = \min_{e \in E_v \cup E_s}{ g_{e}(t)}. $$
% 
% Additionally we record the output of $g_{e_i}(t)$ as the time at
% which we reached $e_i$. We then call DIJKSTRA\_TIME recursively with the
% current node, and explored nodes.
%  Let $P' = P\ \backslash\ \{(x,y)\}$.
% traversing an edge always adds to the time, there is no edge which will take us back
% in time between $x$ and $u$, 

\rmkstar Since $g$ is strictly increasing, any path that arrives to an arbitrary node
later than another path with the same tail will always arrive later.

\proof
If $|S| = 1$, $t = 0$.
Suppose for $|S| = k \geq 1$ that for each node $\in S$ $t(u)$ is the shortest path from $s$
to $u$ for any $u$ in $S$. Let $w$ be the next node added to $S$ via edge $(u,w)$. Let
 $\pi(w)$ be the length of the path from $s$ to $w$. It remains to show that this is in
fact shorter than any other path $P$ from $s$ to $w$. Let $(x,y)$ be an edge in $P$
where $x \in S$, but $y \notin S$. Since we always choose the shortest $s$- path, by via $P$
we arrive at $y$ later than we arrived at $u$. Because $g(t) \geq t$ traversing edges
between $x$ and $u$ cannot be any less than the time at $y$. 

\proof DIJKSTRA\_TIME has the same recurrence relation as Dijkstra's. As in Dijkstra's
algorithm, considering the edges from the current node takes constant time for each node.
Since this is the only difference, we apply the Master Theorem just as we did for 
Dijkstra's. It has been shown that at each explored vertex, if we store the time
to each vertex from the root, we can find the minimum in $O(log\ |V|)$ time. Since
we must consider $|V|$ vertices, the time to do this for our graph takes $O(|V|\ log\ |V|)$.
However, to have had the priority queue of vertices in the first place, we will have
had to explore all edges from all explored vertices. Eventually at termination, we
will have explored all edges once (since they have a single originator). Insertion
of the terminating vertex takes $O(log\ |V|)$ time. Therefore, maintaining the
priority queue takes $O(|E|\ log\ |V|)$. Overall, we have an upper bound of:
$ O(|E|\ log\ |V|\ +\ |E|\ log\ |E|) $, but since for connected graphs $V \in O(|E|)$,
we have: DIJKSTRA\_TIME $\in O(|E|\ log\ |E|)$.


\section{} Let $G = G(V,E)$ be an arbitrary, connected, undirected graph
with vertex set $V$ and edge set $E$. Assume that each $e \in E$ has a
distinct positive length length. Define \textit{tediousness} of any path
$P$ in $G$ to be the length of the longest edge in tediousness of any
path $P$ in $G$ to be the length of the longest edge in $P$. Define the
\textit{tediousness} between vertices $u$ and $v$ to be the tediousness
of the least tedious path connecting $u$ and $v$ in $G$.

\textbf{Fun Subgraph Problem (FSP).} Find a subgraph of edges $S \subseteq E$ such that:

\begin{enumerate}[(a)]
  \item $G(V,S)$ is connected.
  \item $\sum_{e \in S} l_e$ is as small as possible.
  \item For every pair of vertices $u,v \in V$, the \textit{tediousness} between $u,v$ in $G(V,S)$ 
    is no greater than the \textit{tediousness} between $u,v$ in $G(V,E)$.
\end{enumerate}

Prove or disprove the following:

\subsection{} If $S$ is the set of edges in the MST of $G$, then condition 5c holds.

\proof Assume towards a contradiction that the \textit{tediousness} of $G'=G(V,S)$ is greater than
that of $G=G(V,E)$. Then, since $G'$ is acyclic the path in $G'$ between two arbitrary
vertices $u$, $v$ is the path which did not have only the smallest of edges. But since
we know that Kruskal's produces a MST, and in Kruscal's we remove the largest edges first,
there can only exist in $G'$ the path between $u$, and $v$ with the smallest of edges

\subsection{} If Proposition 1 is true, then the edges in the MST of $G$ are an optimal
solution to FSP.

\proof If $A \wedge B \Rightarrow C$ and we know from part 1: $A \wedge B \Rightarrow MST$,
and $A \wedge B \wedge C \Rightarrow FSP$. Then clearly $MST \Rightarrow FSP$.

\subsection{} Every optimal solution to FSP must include every edge in the MST of $G$.

\proof Suppose there exist an optimal solution to the FSP which does not contain
every edge in the MST of $G$. Since by (a) and (b) a solution to the FSP is the
MST of $G$, if we remove an edge, the graph is no longer connected $\Rightarrow \Leftarrow$.

\end{document}
