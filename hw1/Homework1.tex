% ----------------------------------------------------------------
% AMS-LaTeX Paper ************************************************
% **** -----------------------------------------------------------
\documentclass{amsart}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{inconsolata}
% \usepackage{xy}
% ----------------------------------------------------------------
\vfuzz2pt % Don't report over-full v-boxes if over-edge is small
\hfuzz2pt % Don't report over-full h-boxes if over-edge is small
% THEOREMS -------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{alg}[thm]{Algorithm}
\theoremstyle{remark}
\newtheorem{rmk}[thm]{Remark}
\numberwithin{equation}{section}
% MATH -----------------------------------------------------------
\newcommand{\Matrix}[4]{ \left( \begin{array}{cc}  #1 & #2 \\  #3 & #4 \\ \end{array} \right) }
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\NN}{\mathbb N}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\QQ}{\mathbb Q}
\newcommand{\RR}{\mathbb R}
\newcommand{\CC}{\mathbb C}
\newcommand{\isom}{\cong}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\pagestyle{plain}
% ----------------------------------------------------------------
\begin{document}
\title[]{Algorithms Homework 1}%
\author{Evan Simmons \\
        Dept. of Mathematics \& Computer Science \\ University of California Santa Cruz}%
%\date{}
%\dedicatory{}%
%\commby{}%
\renewcommand{\abstractname}{Homework Option}
% ----------------------------------------------------------------
\begin{abstract}
I would like to choose the homework heavy option.
\end{abstract}
\maketitle
% ----------------------------------------------------------------

\section{} 
XXX: REVIEW

Suppose we are given a sorted array of $n$ distinct elements
that has been circularly shifted by $k$ elements. We want to find an
algorithm which runs in $O(log(n))$ time to find the maximum.

\lem \label{lem1}

Consider two elements in our circularly sorted array, $A[i]$ and $A[j]$
where $i < j$. The elements in $A[i..j]$ are sorted if
and only if $A[i] < A[j]$.

\proof{} 

Suppose the array slice $A[i..j]$ is not sorted, then $\exists k \ni
A[k] > A[k+1]$ where $i \leq k \leq j$. We know then that since the
array is circularly sorted that $\forall x \in A[i..k]$ and $\forall y
\in A[(k+1)..j]$ we have $x > y$. Clearly $A[i] \in A[i..k]$ and $A[j] y
\in A[(k+1)..j]$, therefore $A[i] > A[j]$

To show the converse, we simply note that for any sorted array, the
following holds:

$$ \forall i,j \ni i<j \Rightarrow A[i] < A[j] $$

\cor \label{cor1}

Consider two elements in our array $A[i]$ and $A[j]$ where $i<j$, the maximum
is in in $A[i..(j-1)]$ if and only if $A[i] > A[j]$

\proof 

Suppose $A[i] < A[j]$. Then by ~\ref{lem1}, $A[i..j]$ is sorted. Hence
$A[j]$ is larger than every element in $A[i..(j-1)]$, therefore the maximum
cannot be in $A[i..(j-1)]$.

Conversely suppose $A[i] > A[j]$, then by ~\ref{lem1} $A[i..(j-1)]$ is
not sorted. By the definition of a (not completely sorted) circularly
sorted array it has one consecutive pair of elements that are not
pairwise sorted, that pair is the maximum and minimum.

\alg{(CIRCULAR\_MAX)}

If the array consists of a single element, return that element.

Let $h = \floor{ \frac{n}{2} }$; compare the $A[0]$ and $A[h]$ elements. \\

Case (Less than): return the value of CIRCULAR\_MAX( $A[h..n]$ ) \\

Case (Greater than): return the value of CIRCULAR\_MAX( $A[0..(h-1)]$ ) \\

\proof

If an array consists of a single element, clearly that element must
be the maximum. If the first element $A[0]$ is less than the middle
element $A[h]$ then by ~\ref{cor1} the maximum occurs in the array slice
$A[h..n]$. However if $A[0]$ is greater than $A[h]$, then, again by
~\ref{cor1} the maximum occurs in $A[0..(h-1)]$.

\prop{The proposed algorithm runs in $O(log\ n)$ time.}

\proof

Since at each level of recursion we do only constant work, and are able
to eliminate half of the array, the complexity of the algorithm is
simply the height of the longest branch of the recursion tree. Recall
that the height of a tree is $log_{b} (n)$ where $b$ branching factor
and $n$ is the total number of elements (including those pruned).
Therefore the given algorithm is $\Theta( log n )$

Alternatively, we could have used the Master Theorem. The recurrence
relation is $T(n) = T(\frac{n}{2}) + n^0$ hence Case 2 applies and
yeilds $T(n) = \Theta (\log n)$
\\


\section{}
XXX: REVIEW

If $A$ is an array of $n$ numbers. We wish to find:

$$ \max_{1 \leq j<k \leq n} (A[k] - A[j]).$$

\subsection{} Give an $O( n^2 )$ algorithm.
\alg{$O( n^2 )$}

For each element of the array, find its difference with each subsequent
element. Return the largest (positive) value of these differences.

\subsection{} Provide a more efficient algorithm.

At first I was quite stumped with this question. After all, a maximal
subarray could cross any division point. After looking through my
Undergraduate algorithms book\footnote{Thomas H. Cormen, "Introduction
to Algorithms"}, I discovered an algorithm very similar to what follows.
I found the technique very intersting, and even more so that it is
faster than the brute force method.

\alg{MAX\_DIFF}

If the array taken as argument is empty or contains only a single element, return zero.

Let $h = \ceil{n/2}$ where $n$ is the length of the array. Return the following:

\begin{align*}
  \max(\ & \text{MAX\_DIFF}( A[0..(h-1)] ), \\
        & \text{MAX\_DIFF}( A[h..(n-1)] ), \\
        & \text{MAX\_SPANNING\_DIFF}( h, A)\ )
\end{align*}

where MAX\_SPANNING\_DIFF is defined as follows.

\proof 

Clearly a maximal subarray either includes an element or it does not.
Therfore there are three possible cases: the maximal subarray occurs
left of $h$, the maximal subarray occurs right of $h$, or the maximal
subarray includes $A[h]$. For the first two cases, since we call
MAX\_DIFF recursively, it suffices to show only the third. In the third
case we mererly call MAX\_SPANNING\_DIFF which is proven to be correct
below with its definition, therefore we are done.

\alg{MAX\_SPANNING\_DIFF}

We take as argument an index $h$ and an array $A$. We return what
follows by comparing each element in each half of the array with the
center element $h$.

$$ \max_{0 \leq i \leq h}\ (A[h] - A[i]) + \max_{h \leq j \leq n_b -1}\ (A[j] - A[h]). $$

Which is simply:

$$ A[h] - \min_{0 \leq i \leq h}\ ( A[i] ) + \max_{h \leq j \leq n_b -1}\ ( A[j] ) - A[h] $$

$$ \max_{h \leq j \leq n_b -1}\ ( A[j] ) - \min_{0 \leq i \leq h}\ ( A[i] ). $$

\proof 

It remains to show that the sum of these maximums in fact repesents the
difference between the first and last element of the maximal subarray,
which spans the two arrays passed as argument. Let us first consider the
left maximal subarray.

Suppose towards a contradiction that the algorithm does not produce the
maximal subarray difference. Then there exists some $x \in [0,n_a] \cap
\ZZ$ such that:

$$ A[n_a] - A[x] > A[n_a] - A[i]. $$

But then:

$$ A[x] < A[i]. $$

$\Rightarrow\Leftarrow$

\subsection{} Provide a recurrence relation, and find an upper bound
using the Master Theorem.

The recurrence relation is:

$$ T( n ) = 2T(\tfrac{n}{2}) + n. $$

We split the array in half and find the maximal subarray
difference in both sides. We do this by calling MAX\_DIFF recursively on
the two sides. Additionally we must scan each of the n elements at the current
level to find the maximum spanning differnce.

By invoking the Master Theorem we see that $a=2$, $b=2$, and $c=1$ which
means that Case 2 applies and we have $T( n ) = \Theta( n\ log\ n )$


\section{} 
XXX: REVIEW

Consider the following sorting algorithm: First sort the
first two thirds of the elements in the array. Next sort the last two
thirds of the array. Finally, sort the first two thirds again.

\subsection{} Provide an informal explaination of why this algorithm
returns a sorted array. \\

In short, this works because each partition can allow the swap of up to
half of it's elements. Consider the worst case where each element in the
bottom $\frac{1}{3}$ is strictly larger than the top $\frac{1}{3}$. It
is then possible to move the entire bottom $\frac{1}{3}$ into the top
$\frac{1}{3}$ exactly because its length is half of the subarray hence
can be swapped. Similarly for smallest elements.

\subsection{} Find a recurrence relation for the worst-case running time of this algorithm, then solve it using the Master Theorem.
$$ T(n) = 3 \cdot T( \tfrac{2}{3} n) + n^0. $$

Since $\log_{\frac{3}{2}} {3} > 0$, case 3 of the Master Theorem applies, and we get
$T(n) = \Theta ( n^{log_{\frac{3}{2}} {3}} )$

\section{} 
XXX: REVIEW

Consider a monotonously decreasing function $f : \NN \rightarrow \ZZ$. We want to find:

$$ n = \min (\{i \in \NN \mid f(i) \leq 0 \}).$$ \\ 

Give an $O(log\ n)$ algorithm.

Clearly since we must consider a domain of infinite cardinality we cannot simply sub-divide the domain interval and proceed by computing the result for the constituent halves. Cleverly we consider the bins of exponential size, then binary search through that bin.

\alg{SEARCH\_BIN}

This is a simple binary search which takes as argument a lower bound
$x_1$, an upper bound$x_2$, and a mapping $f$ from which to map the
"indices" to values. We seek to find the least index $x \in \NN$ such
that $f(x) < 0$.

If $x_1 = x_2$, return $x_1$.

Let $h = \floor{\tfrac{x_1 + x_2}{2}}$. If $A[h] < 0$, return
SEARCH\_BIN with arguments $x_1$ and $h$. otherwise return 
SEARCH\_BIN with arguments $h+1$ and $x_2$.

\proof

This is essentially binary search with some slight modifications.
Since the function is know to be strictly decreasing, we know that
one of the following is the case:

\begin{enumerate}

  \item if $f(x) > 0$ then the smallest value of $x$ for which the value
of $f$ is negative occurs at some point in the domain $y>x$.

  \item If $f(x) < 0$ then the smallest value of $x$ for which the value
  of $f$ is negative occurs at some $y \leq x$

\end{enumerate}

Our algorithm only considers the half of the domain in which the first negative
value of $f$ can occur.

\alg{INFLECTION}

Our algorithm takes as argument index $x \in \NN$ and a function $f :
\NN \rightarrow \ZZ$. If $lastx$ is uninitialized, initialize it to $0$
If $x$ is uninitialized, initialize it to $2$. Otherwise save $x$ in the
variable $lastx$ and set $x = x^2$. Compute $f(x)$; if $f(x) < 0$, call
SEARCH\_BINARY with $lastx$, $x$ and $f$ Otherwise call INFLECTION again
with $x$, $f$ and (implicitly) $lastx$

\proof
Suppose that the first negative value $f$ occurs at $n \in \NN$.
Since $f$ is strictly decreasing, we know that for some $x \in \NN$, $f(x) < 0$.
We can find a bin of size at most $2^{log_{2} {n}}$ in which $n$ exists by
squaring at each level of recursion the index for which we examine $f$.

\subsection{Complexity}

Since $f$ is a strictly decreasing function it must eventually cross the
x-axis. Let us assume for consistent notation that this happens at some
(unknown) point called $n$. In our algorithm we square the input $x$
value each time, this results in the inequality: $n \leq 2^k$ where $k$
is the number of recursions into INFLECTION. Hence we find the (clearly
negative) value of $f(2^k)$ where $2^k \geq n$ in $O(log\ n)$ time.
After this we do binary search on the bin for which we know $n$ is in.
And of course as we all know binary search runs in $O( log\ n )$ time.


\section{}
XXX: REVIEW

\subsection{} Describe O(n) time algorithms for solving l-LMPS and r-RMPS

\alg{l-LMPS}

Initialize two pairs \texttt{sum = (A[l], l)} and \texttt{maxSum = (A[l], l)}. Iterate over
the array from $i$ndex $l$ to $n-1$. For each element update the sum so that
\texttt{sum = ((fst\ sum)+A[i], i)} and then set \texttt{maxSum = maxFst(sum, maxSum)}. Finally,
when the iteration is complete return the pair \texttt{maxSum}. \\

\rmk
Let \texttt{fst p} and \texttt{snd p} return the first and second element of a pair \texttt{p} respectively.
Let \texttt{maxFst} take pairs as argument and return the pair which is maximal in its first element.

\proof
Clearly if the algorithm maintains the correct partial sum then since it is paired with
the corresponding index, it will return the correct index. We now show that the partial sum
maintained is correct.

If $l=n-1$ then there is only one element in any possible sum, hence the
LMPS is $A[l]$.
Suppose we are on the $k$th interation of our algorithm and that the
values in \texttt{maxSum} and \texttt{sum} are correct for the subarray $A[l..(l+k-1)]$.

There are three cases to consider:

\begin{enumerate}
  \item When we update \texttt{sum}, its value increases to surpass that of \texttt{maxSum}.
  \item When we update \texttt{sum}, its value increases, yet does not surpass that of \texttt{maxSum}.
  \item When we update \texttt{sum}, its value decreases.
\end{enumerate}

In the first, \texttt{maxSum} is updated to reflect the correct answer.
In the second and third, \texttt{maxSum} is not updated so as not to decrease the maxSum. \\

The proof and algorithm for $r$-RMPS are \textbf{exactly} the same with the exeption that instead
we start at a given $r \in [0,n] \cap \ZZ$ and iterate left, down the array,

\rmk Clearly since we traverse the array only once an perform a constant number of operations
for each element, our algorithm runs in constant time.

\subsection{} Given a $O(n)$ time algorithm for l-LMPS, describe a simple $O(n^2)$ algorithm for MPS.

\alg{MPS}

Given an array with length $n$, we iterate over the array maintaining a
variable \texttt{mps} which holds a pair: the greatest return value of prior calls to l-LMPS,
and the pair of indices which produced it. For example, for the $l$th element with $l$-LMPS
returning \texttt{maxSum = (sum, i)}. We set
\texttt{ mps = maxFirst( mps, ((fst maxSum), l, (snd maxSum)) }. 

\proof

By the definition of $l$-LMPS, in its evaluation we have considered
all partial sums which begin with the $l$th element. We then will have
considered all partial sums exactly when we have evaluated $l$-MPS for
each index. The above algorithm does exactly this.


\subsection{} Given O(n) time algorithms for l-LMPS and r-RMPS describe an O(n log n) divide-and- conquer algorithm for MPS.

\alg{MPS}

If the array passed as argument has only one element, return the pair (element, (0,0)).
Suppose we have an array of length n. Let our pivot $p = \floor{ \frac{n}{2} }$.
Return:

$$ \maxFst (\ MPS( A[0..(p-1)], MPS( A[(p+1)..n], MPSpan(p, A) ) )\ ). $$

\proof

If the array has only one element, clearly we return the correct partial sum.

As in problem \#2 we have three cases:

\begin{enumerate}
  \item The maximal subarray occurs strictly left of the pivot.
  \item The maximal subarray occurs strictly right of the pivot.
  \item The maximal subarray includes the pivot.
\end{enumerate}

In our algorithm we take the maximum partial sum returned by asuming
each of the above cases respectively. We may assume inductively that
\texttt{MPS} is correct for the left and right subarray. It then remains
to show that \texttt{MPSpan}.

\alg{MPSpan}

We take as argument a pivot $p$.
Recall that LMPS and RMPS returns a pair, a sum, and the indecies
We evaluate $p$-LMPS, and $p$-RMPS, and return the sum of the two
partial sums and their respective indices.

\texttt{( (fst RMPS) + (fst LMPS) - A[p], ((snd RMPS, snd LMPS) )}.

\proof

Since in LMPS or RMPS must at least include the pivot, we always will double
count $A[p]$, hence we subtract it from the return sum. The MPS which contains
$p$ may be broken up into its component parts:

\begin{align*}
  &\sum{\text{elements left of $p$}}\ + A[p] +\ \sum{\text{elements right of $p$}}. \\
  \intertext{Which is simply equal to:}
  &\sum{\text{elements left of and including $p$}}\ +\ \sum{\text{elements right of and including $p$}}\ + A[p]
\end{align*}

Which is exactly what our algorithm returns.

\begin{enumerate}

\section{BONUS: No}
XXX: REVIEW

\prop

There exist some denomination $\{ c_1, c_2, \ldots , c_n \}$ (ie $c_1 < c_2 < \ldots < c_n$)
in which the at least one of the following
inequalities does not hold, and yet a greedy algorithm still produces an
optimal solution.

$$ \omega_1 c_1 < c_2,\ \omega_1 c_1 + \omega_2 c_2 < c_3,\ \ldots $$

Where each $\omega_i$ represents the maximum number of the $i$th coin
in any optimal solution.

\proof

Consider $c_1 = 1$, $c_2 = 2$, $c_3 = 3$.
Notice that: $1 \cdot 1 + 2 \cdot 1 = 3$ hence breaking the inequality.

Assume towards a contradiction that the greedy algorithm's solution
is not optimal. Sort the set of coins which each algorithm returns and 
let $G$ and $O$ denote greedy, and optimal's respective coin vectors.
Now consider the disagreement of lowest index ($g_i$ \neq $o_i$) for
dispensing the coinage for the amount $x$.

There are two cases:

\begin{enumerate}
  \item $g_i < o_i$
  \item $g_i > o_i$
\end{enumerate}

If (1), since greedy takes the largest coin without overdispensing, $\sum O > x$.
If (2), again we have cases:

\begin{enumerate}
  \item $g_i = 1$
  \item $g_i = 2$
  \item $g_i = 3$
\end{enumerate}

If (1), $O$ is not a solution since if falls short of dispensing $x$.
If (2), $O$ must contain two coins of value 1 to be a solution, hence is not optimal.
If (3), $0$ must either contain three $c_1$'s which is not optimal. Or must contain an
additional $c_2$ and $c_1$ to the shared prefix, meaning it dispenses one more
coin then greedy, hence is not optimal.

\end{document}
% ----------------------------------------------------------------
